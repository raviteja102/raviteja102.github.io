{
    "version": "https://jsonfeed.org/version/1",
    "title": "www.balavivek.com",
    "description": "",
    "home_page_url": "file:///home/balavivek/Documents/Publii/sites/wwwbalavivekcom/preview",
    "feed_url": "feed.json",
    "user_comment": "",
    "icon": "media/website/logo.png",
    "author": {
        "name": "Balavivek"
    },
    "items": [
        {
            "id": "towards-a-virtual-stuntman.html",
            "url": "towards-a-virtual-stuntman.html",
            "title": "Towards a Virtual Stuntman",
            "summary": " Code accompanying the SIGGRAPH 2018 paper: \"DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills\". The framework uses reinforcement learning to train a simulated humanoid to imitate a variety of motion skills from mocap data. Project page: https://xbpeng.github.io/projects/DeepMimic/index.html States and Actions Reference data creation&hellip;",
            "content_html": "<p><img class=\"post__image\" src=\"media/posts/1/teaser.gif\" alt=\"\" width=\"800\" height=\"287\"></p>\n<p>Code accompanying the SIGGRAPH 2018 paper: \"DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills\". The framework uses reinforcement learning to train a simulated humanoid to imitate a variety of motion skills from mocap data.</p>\n<p><strong>Project page</strong>: <a href=\"Project page: https:/xbpeng.github.io/projects/DeepMimic/index.html\">https://xbpeng.github.io/projects/DeepMimic/index.html</a></p>\n<p><strong> States and Actions</strong></p>\n<ul>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">The state s describes the configuration of the character’s body, with features consisting of the relative positions of each link with respect to the root (designated to be the pelvis), their rotations expressed in quaternions, and their linear and angular velocities. </span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">All features are computed in the character’s local coordinate frame, with the root at the origin and the x-axis along the root link’s facing direction</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Policies trained to achieve additional task objectives, such as walking in a particular direction or hitting a target, are also provided with a goal g, which can be treated in a similarly fashion as the state. </span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">The action a from the policy specifies target orientations for PD controllers at each joint. </span></li>\n</ul>\n<p><strong>Reference data creation</strong></p>\n<p><img class=\"post__image\" src=\"media/posts/1/image10.gif\" alt=\"\" width=\"281\" height=\"211\"><img class=\"post__image\" src=\"media/posts/1/image3.png\" alt=\"\" width=\"412\" height=\"130\"></p>\n<p><strong>Network Architecture</strong></p>\n<ul>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Policy π is represented by a neural network that maps a given state s and goal g to a distribution over action π(a|s,g). </span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">The inputs are processed by two fully-connected layers with 1024, and 512 units each, followed by a linear output layer. ReLU activations are used for all hidden units.</span></li>\n</ul>\n<p><span style=\"font-weight: 400;\">Permalink:</span><span style=\"font-weight: 400;\"> <a href=\"#INTERNAL_LINK#/post/null\">http://tiny.cc/qgwlfz</a></span></p>\n<p><strong>Reward  <img class=\"post__image\" src=\"media/posts/1/image1.png\" alt=\"\" width=\"139\" height=\"29\"></strong></p>\n<ul>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Pose Reward</span>\n<ul>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Pose reward r^p  encourages the character to match the joint orientations of the reference motion at each step.</span></li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">It is computed as the difference between the joint orientation quaternions of the simulated character and those of the reference motion. </span></li>\n<li style=\"font-weight: 400;\"><img class=\"post__image\" src=\"media/posts/1/image4.png\" alt=\"\" width=\"248\" height=\"73\"></li>\n</ul>\n</li>\n</ul>\n<ul>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">Velocity Reward</span>\n<ul>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">The velocity reward r^v is computed from the difference of local joint velocities, with angular velocity of the jth joint. The target velocity is computed from the data via finite difference.</span></li>\n<li style=\"font-weight: 400;\"><img class=\"post__image\" src=\"media/posts/1/image6.png\" alt=\"\" width=\"265\" height=\"77\"></li>\n</ul>\n</li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">End Effector Reward</span>\n<ul>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">The end-effector reward ret encourages the character’s hands and feet to match the positions from the reference motion.</span></li>\n<li style=\"font-weight: 400;\"><img class=\"post__image\" src=\"media/posts/1/image2.png\" alt=\"\" width=\"276\" height=\"73\"></li>\n</ul>\n</li>\n<li style=\"font-weight: 400;\"><span style=\"font-weight: 400;\">r^c penalizes deviations in the character’s center-of-mass</span>\n<ul>\n<li style=\"font-weight: 400;\"><img class=\"post__image\" src=\"media/posts/1/image7.png\" alt=\"\" width=\"238\" height=\"56\"></li>\n</ul>\n</li>\n</ul>\n<p><img class=\"post__image\" src=\"media/posts/1/image5.png\" alt=\"Permalink:https://github.com/bsivanantham/DeepMimic/blob/3c7f2171bb09512bee47d667de39761cde9b2d53/DeepMimicCore/scenes/SceneImitate.cpp#L130\" width=\"588\" height=\"142\"></p>\n<p><span style=\"font-weight: 400;\">Permalink:</span><span style=\"font-weight: 400;\"> <a href=\"#INTERNAL_LINK#/post/null\">http://tiny.cc/ahwlfz</a></span></p>\n<p><strong>Results</strong></p>\n<p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/8QFzD-xEKzU\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"allowfullscreen\" ></iframe></p>\n<p>Code: <a href=\"#INTERNAL_LINK#/post/null\">https://github.com/bsivanantham/DeepMimic</a></p>\n<p>Completely trained model: <a href=\"#INTERNAL_LINK#/post/null\">http://tiny.cc/aiwlfz</a></p>\n<p><strong>Reference</strong></p>\n<p>The paper <a href=\"https://xbpeng.github.io/projects/DeepMimic/index.html\" class=\"eq cf gy gz ha hb\" target=\"_blank\" rel=\"noopener noreferrer\"><em class=\"he\">DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills</em></a> has opened up exciting possibilities for improving visual demonstration with highly dynamic character skills.</p>\n<p>The DeepMimic open-sourced codes, data, and frameworks are available on <a href=\"https://github.com/bsivanantham/DeepMimic\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a>.</p>",
            "image": "media/posts/1/teaser.gif",
            "author": {
                "name": "Balavivek"
            },
            "tags": [
            ],
            "date_published": "2019-11-02T20:57:00+01:00",
            "date_modified": "2019-11-02T21:00:13+01:00"
        }
    ]
}
